---
title: kafka_server
slug: kafka_server
type: input
status: beta
categories: ["Services"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution BETA
This component is mostly stable but breaking changes could still be made outside of major version releases if a fundamental problem with the component is found.
:::
Runs a Kafka-protocol-compatible server that accepts produce requests from Kafka producers.

Introduced in version 1.15.0.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yml
# Common config fields, showing default values
input:
  label: ""
  kafka_server:
    address: 0.0.0.0:9092
```

</TabItem>
<TabItem value="advanced">

```yml
# All config fields, showing default values
input:
  label: ""
  kafka_server:
    address: 0.0.0.0:9092
    advertised_address: ""
    topics: []
    cert_file: ""
    key_file: ""
    mtls_auth: ""
    mtls_cas_files: []
    sasl: [] # No default (optional)
    timeout: 5s
    idle_timeout: "0"
    max_message_bytes: 1048576
    idempotent_write: false
```

</TabItem>
</Tabs>

This input acts as a Kafka broker endpoint, allowing Kafka producers to send messages directly into a Bento pipeline without requiring a full Kafka cluster.

Similar to the `http_server` input, this creates a server that external clients can push data to. The difference is that clients use the Kafka protocol instead of HTTP.

### Metadata

This input adds the following metadata fields to each message:

```text
- kafka_server_topic
- kafka_server_partition
- kafka_server_key
- kafka_server_timestamp
- kafka_server_timestamp_unix
- kafka_server_client_address
```

Message headers from Kafka records are also added as metadata fields.

You can access these metadata fields using [function interpolation](/docs/configuration/interpolation#bloblang-queries).

## Examples

<Tabs defaultValue="Basic Usage" values={[
{ label: 'Basic Usage', value: 'Basic Usage', },
{ label: 'With TLS', value: 'With TLS', },
{ label: 'With mTLS', value: 'With mTLS', },
{ label: 'With mTLS (multiple CAs)', value: 'With mTLS (multiple CAs)', },
{ label: 'With mTLS (optional verification)', value: 'With mTLS (optional verification)', },
{ label: 'With SASL PLAIN Authentication', value: 'With SASL PLAIN Authentication', },
{ label: 'With SASL SCRAM Authentication', value: 'With SASL SCRAM Authentication', },
]}>

<TabItem value="Basic Usage">

Accept Kafka produce requests and write to stdout

```yaml
input:
  kafka_server:
    address: "0.0.0.0:9092"
    topics:
      - events
      - logs

output:
  stdout: {}
```

</TabItem>
<TabItem value="With TLS">

Accept Kafka produce requests over TLS

```yaml
input:
  kafka_server:
    address: "0.0.0.0:9093"
    cert_file: /path/to/server-cert.pem
    key_file: /path/to/server-key.pem
```

</TabItem>
<TabItem value="With mTLS">

Accept Kafka produce requests with mutual TLS authentication

```yaml
input:
  kafka_server:
    address: "0.0.0.0:9093"
    cert_file: /path/to/server-cert.pem
    key_file: /path/to/server-key.pem
    mtls_auth: require_and_verify
    mtls_cas_files:
      - /path/to/client-ca.pem
```

</TabItem>
<TabItem value="With mTLS (multiple CAs)">

Accept Kafka produce requests with multiple client certificate authorities

```yaml
input:
  kafka_server:
    address: "0.0.0.0:9093"
    cert_file: /path/to/server-cert.pem
    key_file: /path/to/server-key.pem
    mtls_auth: require_and_verify
    mtls_cas_files:
      - /path/to/client-ca-1.pem
      - /path/to/client-ca-2.pem
```

</TabItem>
<TabItem value="With mTLS (optional verification)">

Accept Kafka produce requests with optional client certificate verification

```yaml
input:
  kafka_server:
    address: "0.0.0.0:9093"
    cert_file: /path/to/server-cert.pem
    key_file: /path/to/server-key.pem
    mtls_auth: verify_if_given
    mtls_cas_files:
      - /path/to/client-ca.pem
```

</TabItem>
<TabItem value="With SASL PLAIN Authentication">

Accept authenticated Kafka produce requests using PLAIN

```yaml
input:
  kafka_server:
    address: "0.0.0.0:9092"
    cert_file: /path/to/server-cert.pem
    key_file: /path/to/server-key.pem
    sasl:
      - mechanism: PLAIN
        username: producer1
        password: secret123
      - mechanism: PLAIN
        username: producer2
        password: secret456
```

</TabItem>
<TabItem value="With SASL SCRAM Authentication">

Accept authenticated Kafka produce requests using SCRAM-SHA-256

```yaml
input:
  kafka_server:
    address: "0.0.0.0:9092"
    cert_file: /path/to/server-cert.pem
    key_file: /path/to/server-key.pem
    sasl:
      - mechanism: SCRAM-SHA-256
        username: producer1
        password: secret123
      - mechanism: SCRAM-SHA-512
        username: producer2
        password: secret456
```

</TabItem>
</Tabs>

## Fields

### `address`

The address to listen on for Kafka protocol connections.


Type: `string`  
Default: `"0.0.0.0:9092"`  

### `advertised_address`

The address to advertise to clients in metadata responses. If empty, the listen address is used. This is useful when the server is behind a NAT or load balancer, or when clients connect through a different network interface (e.g., Docker containers connecting via host.docker.internal).


Type: `string`  
Default: `""`  

### `topics`

Optional list of topic names to accept. If empty, all topics are accepted.


Type: `array`  
Default: `[]`  

### `cert_file`

An optional server certificate file for enabling TLS.


Type: `string`  
Default: `""`  

### `key_file`

An optional server key file for enabling TLS.


Type: `string`  
Default: `""`  

### `mtls_auth`

Sets the policy the server will follow for mTLS client authentication. Only used when TLS is enabled.


Type: `string`  
Default: `""`  

| Option | Summary |
|---|---|
| `none` | Server will not request a client certificate |
| `request` | Server will request a client certificate but doesn't require the client to send one |
| `require` | Server will require any client certificate (doesn't verify it) |
| `require_and_verify` | Server requires a client certificate and will verify it against the mtls_cas_files |
| `verify_if_given` | Server will request a client certificate and verify it if provided |


### `mtls_cas_files`

An optional list of paths to files containing client certificate authorities to use for verifying client certificates. Only used when mtls_auth is set to verify_if_given or require_and_verify.


Type: `array`  
Default: `[]`  

### `sasl`

Configure one or more SASL credentials that clients can use to authenticate. When SASL is configured, clients must authenticate before sending produce requests. Multiple credentials can be configured for the same or different mechanisms.


Type: `array`  

```yml
# Examples

sasl:
  - mechanism: PLAIN
    password: password1
    username: user1
  - mechanism: SCRAM-SHA-256
    password: password2
    username: user2
```

### `sasl[].mechanism`

The SASL mechanism to use for this credential.


Type: `string`  

| Option | Summary |
|---|---|
| `PLAIN` | Plain text authentication. Credentials are sent in clear text, so TLS is recommended. |
| `SCRAM-SHA-256` | SCRAM based authentication as specified in RFC5802. |
| `SCRAM-SHA-512` | SCRAM based authentication as specified in RFC5802. |


### `sasl[].username`

The username for authentication.


Type: `string`  

### `sasl[].password`

The password for authentication.
:::warning Secret
This field contains sensitive information that usually shouldn't be added to a config directly, read our [secrets page for more info](/docs/configuration/secrets).
:::


Type: `string`  

### `timeout`

The maximum time to wait for a produce request to be processed by the pipeline and acknowledged. This timeout also applies to write operations when sending responses back to clients. If processing takes longer than this duration, the producer receives a timeout error, but the message may still be delivered to the pipeline.


Type: `string`  
Default: `"5s"`  

### `idle_timeout`

The maximum time a connection can be idle (no data received) before being closed. Use a larger value for producers that send infrequently. Set to 0 to disable idle timeout.


Type: `string`  
Default: `"0"`  

### `max_message_bytes`

The maximum size in bytes of a message payload.


Type: `int`  
Default: `1048576`  

### `idempotent_write`

Enable support for idempotent Kafka producers. When enabled, the server will respond to InitProducerID requests and allocate producer IDs. Note: This enables clients to use idempotent producers but does NOT provide actual exactly-once semantics - it only satisfies the protocol requirements.


Type: `bool`  
Default: `false`  


